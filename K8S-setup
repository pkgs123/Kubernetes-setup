Reference URL: https://nvtienanh.info/blog/cai-dat-kubernetes-cluster-tren-ubuntu-server-22-04
Addon-> https://www.youtube.com/watch?v=o6bxo0Oeg6o

When we are running on any device, we are operating on any device that is updated to the latest version (which is available on all nodes).

sudo apt update
sudo apt upgrade -y

After the update is complete, we restart the server.
sudo reboot

Set hostname and update hosts file
Master node
sudo hostnamectl set-hostname "k8s-master.nvtienanh.local"

Worker 1
sudo hostnamectl set-hostname "k8s-worker1.nvtienanh.local"

Worker 2

sudo hostnamectl set-hostname "k8s-worker2.nvtienanh.local"

Next we update the /etc/hosts file of all nodes.
sudo apt install -y nano // Cài nano editor nếu server chưa có
sudo nano /etc/hosts

Then add the file contents below:
172.31.95.191  k8s-master.prashant.local
172.31.94.148  k8s-worker1.prashant.local



Disable swap and update kernel
Please note:
The commands below are executed on all nodes.
sudo swapoff -a
Check again to see if swap is disabled and not by command free -h, if successful the result will be as below:

$ free -h
               
total        used        free      shared  buff/cache   available
Mem:           7.7Gi       167Mi       7.1Gi       1.0Mi       437Mi   7.3Gi
Swap:             0B          0B          0B

Next disable swap in /etc/fstab
sudo nano /etc/fstab

Find this line: /swap.img none swap sw 0 0 and update again:
#/swap.img       none       swap       sw       0       0

Then run these commands:
sudo mount -a
free -h

Load the following kernel modules on all the nodes:
sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

Set the following Kernel parameters for Kubernetes.

sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

Then reload sysctl

sudo sysctl --system
Install containerd run time
Please note:

What are the commands below on all nodes

sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt update
sudo apt install -y containerd.io

After the installation is complete, we add more configuration containerd.

containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

Install Kubernetes
Please note:

What are the commands below on all nodes

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

Create cluster using kubeadm
Please note:

Just run down here on your master node

sudo kubeadm init \
  --pod-network-cidr=10.10.0.0/16 \
  --control-plane-endpoint= k8s-master.prashant.local

While 10.10.0.0/16 is the CIDR of your pod network, you can change it as per your requirement.

If run successfully, the result returned will be as follows


Next, execute these commands below on your master node

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Let's run this command to find out the status of your cluster:

kubectl cluster-info
kubectl get nodes

Result returned:
 

anh@k8s-master:~$ kubectl cluster-info
Kubernetes control plane is running at https://k8s-master.nvtienanh.local:6443
CoreDNS is running at https://k8s-master.nvtienanh.local:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
anh@k8s-master:~$ kubectl get nodes
NAME                         STATUS     ROLES           AGE   VERSION
k8s-master.nvtienanh.local   NotReady   control-plane   55m   v1.26.1

We see that the control plane is running and currently only each master node is there, we will move to add more worker nodes to this cluster.

Add worker node to cluster
Note: The commands below only run on each worker node.

Let's see the output of your command sudo kubeadm init and copy the command below:

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s-master.nvtienanh.local:6443 --token daii9y.g4dq24u6irkz4pt0 \
        --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148

The result returned to your terminal will look like this

anh@k8s-worker1:~$ sudo kubeadm join k8s-master.nvtienanh.local:6443 --token daii9y.g4dq24u6irkz4pt0 \
        --discovery-token-ca-cert-hash sha256:58b9cc96ed57a5797fddea653756dbda830efbff55b720a10cffb3948d489148
[sudo] password for anh:
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


When I run kubectl get nodes to find each node (check my master node's terminal), I get the following result:

anh@k8s-master:~$ kubectl get nodes
NAME                          STATUS     ROLES           AGE   VERSION
k8s-master.nvtienanh.local    NotReady   control-plane   58m   v1.26.1
k8s-worker1.nvtienanh.local   NotReady   <none>          87s   v1.26.1
k8s-worker2.nvtienanh.local   NotReady   <none>          44s   v1.26.1

Shows which nodes have been successfully added to the cluster.

Install Calico Pod Network for Kubernetes cluster
Note: The commands below only run on the master node.

First we download the manifest file as yaml file, this is the file installed on Calico on Kubernetes cluster with number of nodes less than 50.
 

curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

Open the file you just downloaded and find the section CALICO_IPV4POOL_CIDR.
---
# The default IPv4 pool to create on startup if none exists. Pod IPs will be
# chosen from this range. Changing this value after installation will have
# no effect. This should fall within `--cluster-cidr`.
# - name: CALICO_IPV4POOL_CIDR
#   value: "192.168.0.0/16"
# Disable file logging so `kubectl logs` works.
- name: CALICO_DISABLE_FILE_LOGGING
  value: 'true'

Let's change the IP range to match the CIDR of your pod network with the command sudo kubeadm init. In case yours is 10.10.0.0/16 then the file when modified should look like this

---
# The default IPv4 pool to create on startup if none exists. Pod IPs will be
# chosen from this range. Changing this value after installation will have
# no effect. This should fall within `--cluster-cidr`.
- name: CALICO_IPV4POOL_CIDR
  value: '10.10.0.0/16'
# Disable file logging so `kubectl logs` works.
- name: CALICO_DISABLE_FILE_LOGGING
  value: 'true'

Then we added Calico to the Kubernetes cluster:

kubectl apply -f calico.yaml

We will check which pods are deployed in our namespace kube-system.

kubectl get pods -n kube-system
anh@k8s-master:~$ kubectl get pods -n kube-system
NAME                                                 READY   STATUS    RESTARTS   AGE
calico-kube-controllers-57b57c56f-ptddp              1/1     Running   0          2m44s
calico-node-5fqml                                    1/1     Running   0          2m44s
calico-node-llfjq                                    1/1     Running   0          2m44s
calico-node-vw78h                                    1/1     Running   0          2m44s
coredns-787d4945fb-n7s9t                             1/1     Running   0          62m
coredns-787d4945fb-rs9mj

If the status is Running that means the deployment has completed successfully, now if you check the status of each node then the status will be Ready.
 

anh@k8s-master:~$ kubectl get nodes
NAME                          STATUS   ROLES           AGE     VERSION
k8s-master.nvtienanh.local    Ready    control-plane   62m     v1.26.1
k8s-worker1.nvtienanh.local   Ready    <none>          5m53s   v1.26.1
k8s-worker2.nvtienanh.local   Ready    <none>          5m10s   v1.26.1
